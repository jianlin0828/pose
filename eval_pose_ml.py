#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import os
import sys
import argparse
import math
import csv
from pathlib import Path
import numpy as np
from PIL import Image
import torch
import torchvision.transforms as transforms
from scipy.spatial.transform import Rotation
import joblib  # å¼•å…¥ joblib ä¾†è¼‰å…¥æ©Ÿå™¨å­¸ç¿’æ¨¡å‹

# ==========================================
# 1. ç³»çµ±è¨­å®šèˆ‡æ¨™è¨»å°ç…§
# ==========================================
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)

# æ–°ç‰ˆ GT æ¨™è¨»è¡¨ (åƒ…ç”¨æ–¼å¯«å…¥ GT æ¬„ä½ä¾›åƒè€ƒï¼Œä¸å½±éŸ¿ AI åˆ¤æ–·)
PROMPT_TO_GT = {
    "turns her head back over her shoulder": "Back_Over_Shoulder",
    "turns her head over her right shoulder": "Back_Over_Shoulder",
    "turns her head left": "Head_Turn_Left",
    "looks sideways toward the left": "Head_Turn_Left",
    "turns his head right": "Head_Turn_Right",
    "looks to his right": "Head_Turn_Right",
    "turns his head slightly to the right": "Head_Slight_Right",
    "tilts his head left": "Head_Tilt_Left",
    "head tilted right": "Head_Tilt_Right",
    "leans his head toward his right shoulder": "Head_Tilt_Right",
    "looks straight": "Frontal",
    "tilts her head downward": "Frontal",
    "faces downward": "Frontal",
    "faces slightly downward": "Frontal",
    "looks down to her left": "Head_Slight_Left",
    "looks upward, head tilted back": "Frontal",
    "looks upward": "Frontal",
    "tilts her head backward": "Frontal",
    "looks up and to his left": "Head_Slight_Left",
    "turns his face upward to the left": "Head_Slight_Left",
}

HAS_MEDIAPIPE = False
try:
    import mediapipe as mp
    if hasattr(mp, 'solutions'):
        HAS_MEDIAPIPE = True
except ImportError:
    pass

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
IDX_L_SHOULDER = 11
IDX_R_SHOULDER = 12
FACE_LANDMARKS_INDICES = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
SUPPORT_EXT = {".jpg", ".jpeg", ".png", ".bmp", ".webp"}

class SOTAConfig:
    def __init__(self):
        self.num_classes = 9 

# ==========================================
# 2. è¼”åŠ©å‡½æ•¸
# ==========================================
def normalize_angle(angle):
    if angle is None: return 0.0 # ML æ¨¡å‹éœ€è¦æ•¸å€¼ï¼ŒNone è£œ 0
    angle = float(angle)
    while angle > 180: angle -= 360
    while angle < -180: angle += 360
    return angle

def limit_angle(angle):
    while angle < -180: angle += 360
    while angle > 180: angle -= 360
    return angle

def load_head_model(checkpoint_path):
    print(f"ğŸ“‚ æ­£åœ¨è§£æ SemiUHPE æ¬Šé‡æª”: {checkpoint_path}")
    try:
        from src.networks import get_EfficientNet_V2
        from src.fisher.fisher_utils import batch_torch_A_to_R  # ç¢ºä¿é€™è¡Œä¸æœƒå ±éŒ¯ï¼Œè‹¥æ²’æœ‰è«‹è‡ªè¡Œèª¿æ•´ import
        
        config = SOTAConfig()
        model = get_EfficientNet_V2(config, model_name="S")
        checkpoint = torch.load(checkpoint_path, map_location=DEVICE)
        
        # è™•ç† key çš„å‰ç¶´
        state_dict = checkpoint.get('model_state_dict_ema', checkpoint.get('model_state_dict', checkpoint))
        new_state_dict = {k.replace("module.", ""): v for k, v in state_dict.items()}
        
        model.load_state_dict(new_state_dict, strict=True)
        model.to(DEVICE)
        model.eval()
        return model
    except Exception as e:
        print(f"âŒ SemiUHPE æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
        return None

def compute_pose_output(output_tensor):
    # é€™è£¡ç°¡åŒ–è™•ç†ï¼Œå‡è¨­å·²æœ‰ batch_torch_A_to_R
    try:
        from src.fisher.fisher_utils import batch_torch_A_to_R
        with torch.no_grad():
            rot_mat = batch_torch_A_to_R(output_tensor).cpu().numpy()[0]
            rot_mat_2 = np.transpose(rot_mat)
            r = Rotation.from_matrix(rot_mat_2)
            angles = r.as_euler("xyz", degrees=True)
            return limit_angle(angles[1]), limit_angle(angles[0] - 180), limit_angle(angles[2])
    except:
        return 0.0, 0.0, 0.0

def get_face_box_from_pose(landmarks, w, h):
    x_coords = [landmarks[i].x * w for i in FACE_LANDMARKS_INDICES]
    y_coords = [landmarks[i].y * h for i in FACE_LANDMARKS_INDICES]
    if not x_coords: return None
    min_x, max_x = min(x_coords), max(x_coords)
    min_y, max_y = min(y_coords), max(y_coords)
    box_size = max(max_x - min_x, max_y - min_y) * 1.5
    cx, cy = (min_x + max_x) / 2, (min_y + max_y) / 2
    return [int(cx - box_size/2), int(cy - box_size/2), int(cx + box_size/2), int(cy + box_size/2)]

def calc_body_yaw(landmarks):
    l_sh = landmarks[IDX_L_SHOULDER]
    r_sh = landmarks[IDX_R_SHOULDER]
    if l_sh.visibility < 0.5 or r_sh.visibility < 0.5: return None
    dx, dz = r_sh.x - l_sh.x, r_sh.z - l_sh.z
    return -math.degrees(math.atan2(dz, dx)) * 2.0 

def calc_body_roll(landmarks, width, height):
    l_sh = landmarks[IDX_L_SHOULDER]
    r_sh = landmarks[IDX_R_SHOULDER]
    if l_sh.visibility < 0.5 or r_sh.visibility < 0.5: return 0.0
    lx, ly = l_sh.x * width, l_sh.y * height
    rx, ry = r_sh.x * width, r_sh.y * height
    return math.degrees(math.atan2(ly - ry, lx - rx))

# ==========================================
# 3. ä¸»ç¨‹å¼
# ==========================================
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--img-dir", required=True, help="åœ–ç‰‡è³‡æ–™å¤¾")
    parser.add_argument("--out-dir", required=True, help="è¼¸å‡ºçµæœè³‡æ–™å¤¾")
    parser.add_argument("--checkpoint", required=True, help="SemiUHPE æ¨¡å‹æ¬Šé‡ (.pth)")
    parser.add_argument("--ml-model", required=True, help="æ©Ÿå™¨å­¸ç¿’æ±ºç­–æ¨¡å‹ (.pkl)")
    parser.add_argument("--prompts-file", required=False, help="CSVæª”æ¡ˆï¼ŒåŒ…å« filename èˆ‡ prompt")
    args = parser.parse_args()

    if not HAS_MEDIAPIPE:
        print("âŒ éŒ¯èª¤: æœªå®‰è£ MediaPipe (pip install mediapipe)")
        return

    # 1. è¼‰å…¥ Prompt å°ç…§è¡¨
    prompt_dict = {}
    if args.prompts_file and os.path.exists(args.prompts_file):
        print(f"ğŸ“– è®€å– Prompt æª”: {args.prompts_file}")
        with open(args.prompts_file, 'r', encoding='utf-8') as f:
            reader = csv.reader(f)
            next(reader, None) 
            for row in reader:
                if len(row) >= 2:
                    prompt_dict[row[0].strip()] = row[1].strip()

    # 2. è¼‰å…¥ ML åˆ†é¡æ¨¡å‹ (Decision Tree / Random Forest)
    print(f"ğŸ¤– è¼‰å…¥æ±ºç­–æ¨¡å‹: {args.ml_model}")
    if os.path.exists(args.ml_model):
        pose_classifier = joblib.load(args.ml_model)
    else:
        print("âŒ éŒ¯èª¤: æ‰¾ä¸åˆ° .pkl æ¨¡å‹æª”æ¡ˆï¼")
        return

    # 3. åˆå§‹åŒ–å·¥å…·
    img_dir = Path(args.img_dir)
    out_dir = Path(args.out_dir)
    out_dir.mkdir(parents=True, exist_ok=True)
    csv_path = out_dir / "pose_classification_ml.csv"

    mp_pose = mp.solutions.pose
    pose_detector = mp_pose.Pose(static_image_mode=True, model_complexity=2, min_detection_confidence=0.5)
    head_model = load_head_model(args.checkpoint)
    
    files = sorted([p for p in img_dir.rglob('*') if p.suffix.lower() in SUPPORT_EXT])
    print(f"ğŸ” æ‰¾åˆ° {len(files)} å¼µåœ–ç‰‡")

    # 4. é–‹å§‹è™•ç†
    with open(csv_path, mode='w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        # æ¨™é¡Œæ–°å¢ Confidence (å¦‚æœæ¨¡å‹æ”¯æ´ predict_proba)
        writer.writerow(["Filename", "Prompt", "GT_Pose", "AI_Prediction", "Raw_Angles(BY/BP/BR/HY/HP/HR)"])

        for idx, p in enumerate(files):
            try:
                img_pil = Image.open(p).convert("RGB")
                W, H = img_pil.size
                img_arr = np.array(img_pil)
                
                # A. MediaPipe æå–èº«é«”ç‰¹å¾µ
                results = pose_detector.process(img_arr)
                
                raw_body_yaw = None
                raw_body_roll = 0.0
                h_yaw, h_pitch, h_roll = 0.0, 0.0, 0.0
                norm_body_yaw = 0.0
                
                bbox = None

                if results.pose_landmarks:
                    lm = results.pose_landmarks.landmark
                    raw_body_yaw = calc_body_yaw(lm)
                    raw_body_roll = calc_body_roll(lm, W, H)
                    norm_body_yaw = normalize_angle(raw_body_yaw)
                    bbox = get_face_box_from_pose(lm, W, H)

                # B. SemiUHPE æå–é ­éƒ¨ç‰¹å¾µ
                if bbox and head_model:
                    x1, y1, x2, y2 = bbox
                    crop = img_pil.crop((max(0, x1), max(0, y1), min(W, x2), min(H, y2)))
                    if crop.size[0] > 5 and crop.size[1] > 5:
                        tf = transforms.Compose([
                            transforms.Resize((224, 224)), transforms.ToTensor(),
                            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
                        ])
                        input_t = tf(crop).unsqueeze(0).to(DEVICE)
                        with torch.no_grad():
                            out = head_model(input_t)
                            h_yaw, h_pitch, h_roll = compute_pose_output(out)
                
                # C. æº–å‚™ç‰¹å¾µå‘é‡ (Feature Vector)
                # è¨“ç·´æ™‚çš„ç‰¹å¾µé †åº: [BodyYaw, BodyPitch, BodyRoll, HeadYaw, HeadPitch, HeadRoll]
                # æ³¨æ„: MediaPipe 2D æ²’æœ‰ Body Pitchï¼Œæ•…è£œ 0.0
                
                prediction = "Unknown"
                
                if raw_body_yaw is None:
                    prediction = "No_Body_Detected"
                else:
                    # æ§‹å»º 1x6 çš„ç‰¹å¾µé™£åˆ—
                    features = np.array([[
                        norm_body_yaw,   # Body Yaw
                        0.0,             # Body Pitch (MP 2D ä¸æ”¯æ´ï¼Œè£œ 0)
                        raw_body_roll,   # Body Roll
                        h_yaw,           # Head Yaw
                        h_pitch,         # Head Pitch
                        h_roll           # Head Roll
                    ]])
                    
                    # D. AI æ¨¡å‹é æ¸¬
                    prediction = pose_classifier.predict(features)[0]

                # E. å–å¾— Prompt å°ç…§ (åƒ…ä¾›åƒè€ƒ)
                prompt_text = prompt_dict.get(p.name, "")
                gt_pose = "Unknown"
                for key_prompt, val_gt in PROMPT_TO_GT.items():
                    if key_prompt.lower() in prompt_text.lower():
                        gt_pose = val_gt
                        break

                # F. å¯«å…¥çµæœ
                angle_str = f"{norm_body_yaw:.1f}/0.0/{raw_body_roll:.1f}/{h_yaw:.1f}/{h_pitch:.1f}/{h_roll:.1f}"
                
                writer.writerow([
                    p.name, 
                    prompt_text, 
                    gt_pose, 
                    prediction,
                    angle_str
                ])

                if idx % 50 == 0: print(f"è™•ç†ä¸­: {idx}/{len(files)} -> {prediction}")

            except Exception as e:
                print(f"Error processing {p.name}: {e}")

    print(f"\nâœ… å®Œæˆï¼çµæœå·²å­˜è‡³: {csv_path}")

if __name__ == "__main__":
    main()
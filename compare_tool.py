import gradio as gr
import os
import csv
import math
import torch
import cv2
import pandas as pd
import numpy as np
from PIL import Image, ImageDraw, ImageFont
import torchvision.transforms as transforms
from scipy.spatial.transform import Rotation
import sys
import mediapipe as mp

# --- 1. è¨­å®šèˆ‡æ¨¡å‹è¼‰å…¥ ---

# è«‹ä¿®æ”¹ç‚ºæ‚¨çš„æ¬Šé‡è·¯å¾‘
CHECKPOINT_PATH = "checkpoints/DAD-WildHead-EffNetV2-S-best.pth" 
# å¦‚æœ src åœ¨ç•¶å‰ç›®éŒ„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# ç’°å¢ƒè¨­å®š
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
IDX_L_SHOULDER = 11
IDX_R_SHOULDER = 12
FACE_LANDMARKS = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]

# å˜—è©¦è¼‰å…¥ä¾è³´
try:
    from src.networks import get_EfficientNet_V2
    from src.fisher.fisher_utils import batch_torch_A_to_R
    HAS_DEPS = True
    mp_pose = mp.solutions.pose
except ImportError:
    HAS_DEPS = False
    print("âŒ ç¼ºå°‘å¿…è¦å¥—ä»¶ (src, mediapipe)ï¼Œè«‹æª¢æŸ¥ç’°å¢ƒã€‚")

class SOTAConfig:
    def __init__(self):
        self.num_classes = 9

# --- æ¨¡å‹è¼‰å…¥å‡½æ•¸ ---
def load_models(checkpoint_path):
    if not HAS_DEPS: return None, None
    print(f"ğŸ“‚ è¼‰å…¥æ¨¡å‹æ¬Šé‡: {checkpoint_path}")
    try:
        # Load Head Model
        config = SOTAConfig()
        head_model = get_EfficientNet_V2(config, model_name="S")
        checkpoint = torch.load(checkpoint_path, map_location=DEVICE)
        
        state_dict = checkpoint.get('model_state_dict_ema', checkpoint.get('model_state_dict', checkpoint))
        new_state_dict = {k.replace("module.", ""): v for k, v in state_dict.items()}
        head_model.load_state_dict(new_state_dict, strict=True)
        head_model.to(DEVICE)
        head_model.eval()

        # Load MediaPipe
        pose_estimator = mp_pose.Pose(static_image_mode=True, model_complexity=2, min_detection_confidence=0.5)
        
        return head_model, pose_estimator
    except Exception as e:
        print(f"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
        return None, None

# --- æ ¸å¿ƒè¨ˆç®—é‚è¼¯ ---
def normalize_angle_180(angle):
    if angle is None: return None
    while angle > 180: angle -= 360
    while angle < -180: angle += 360
    return angle

def get_face_box(landmarks, w, h):
    xs = [landmarks[i].x * w for i in FACE_LANDMARKS]
    ys = [landmarks[i].y * h for i in FACE_LANDMARKS]
    if not xs: return None
    x1, x2 = min(xs), max(xs)
    y1, y2 = min(ys), max(ys)
    cx, cy = (x1+x2)/2, (y1+y2)/2
    size = max(x2-x1, y2-y1) * 1.5
    return [int(cx - size/2), int(cy - size/2), int(cx + size/2), int(cy + size/2)]

def calc_angles(lm, w, h):
    l = lm[IDX_L_SHOULDER]
    r = lm[IDX_R_SHOULDER]
    b_yaw = None
    b_roll = 0.0
    if l.visibility > 0.5 and r.visibility > 0.5:
        # ä¿®æ­£å¾Œçš„ Body Yaw è¨ˆç®— (l - r)
        dx = l.x - r.x
        dz = l.z - r.z
        b_yaw = normalize_angle_180(-math.degrees(math.atan2(dz, dx)) )
        
        # Body Roll
        lx_px, ly_px = l.x * w, l.y * h
        rx_px, ry_px = r.x * w, r.y * h
        dx_roll = abs(rx_px - lx_px)
        dy_roll = ry_px - ly_px
        b_roll = normalize_angle_180(math.degrees(math.atan2(dy_roll, dx_roll)))
        
    return b_yaw, b_roll

# --- V-Final-Optimized é‚è¼¯ ---
def classify_custom_priority(b_yaw, b_roll, h_yaw, h_pitch, h_roll, delta):
    if b_yaw is None or h_yaw is None: return "Unknown_Fail"
    
    abs_b_yaw = abs(b_yaw)
    abs_h_yaw = abs(h_yaw)
    
    # --- 1. é–¾å€¼è¨­å®š ---
    
    if b_yaw > 0:
        THRES_BODY_SIDE_START = 35 
    else:
        THRES_BODY_SIDE_START = 20

    THRES_BODY_BACK = 89
    THRES_HEAD_FRONT_LIMIT = 30
    THRES_HEAD_PURE_TURN = 22 
    THRES_LEAN = 5 
    THRES_TILT = 8 

    # -----------------------------------------------

    # --- Priority 1: æ—©æœŸå‚¾æ–œä¿è­· (Early Lean) ---
    # [ä¿®æ­£] æ“´å¤§å®ˆå‚™ç¯„åœï¼šYaw < 40 (åŸ30)
    # åªè¦åœ¨ 40 åº¦ä»¥å…§ä¸”æœ‰æ­ªï¼Œéƒ½ç®—å‚¾æ–œï¼Œé˜²æ­¢æ¼åˆ° Frontal
    if abs_b_yaw < 40 and abs(b_roll) > THRES_LEAN:
        if b_roll > 0: return "Body_Lean_Right"
        else: return "Body_Lean_Left"

    # --- Priority 2: èƒŒå°é¡ (Back View) ---
    if abs_b_yaw > THRES_BODY_BACK:
        if abs(delta) < 40:
            return "Back_View_Straight"
        elif abs_h_yaw < 60: 
            return "Back_Over_Shoulder"
        else:
            return "Back_View_Side_Looking_Away"

    # --- Priority 3: å¼·åˆ¶é ­è½‰ (é™åˆ¶å‹) ---
    if abs_h_yaw > 55 and abs_b_yaw < 60:
         return "Head_Turn_Right" if h_yaw > 0 else "Head_Turn_Left"

    # --- Priority 4: å´å‘å‹•ä½œçŸ©é™£ (Side Matrix) ---
    
    is_body_side = (abs_b_yaw > THRES_BODY_SIDE_START) and (abs_b_yaw <= THRES_BODY_BACK)
    
    if is_body_side:
        # [æ ¸å¿ƒä¿®æ­£] æ™ºæ…§å‹ç¬¦è™Ÿæ ¡æ­£ 2.0 (Smart Sign Correction)
        # ç›®æ¨™ï¼šè§£æ±º Side_View å’Œ Body_Turn çš„å·¦å³é¡åƒå•é¡Œ
        
        final_yaw_direction_sign = 1 if b_yaw > 0 else -1
        
        # åˆ¤æ–·æ˜¯å¦ç™¼ç”Ÿæ–¹å‘è¡çª (Body èˆ‡ Head ç¬¦è™Ÿç›¸å)
        is_conflict = (b_yaw * h_yaw) < 0
        
        # æ¢ä»¶ï¼šå¦‚æœç™¼ç”Ÿè¡çªï¼Œä¸” é ­éƒ¨è½‰å‹•éå¸¸æ˜ç¢º (>40)
        # æˆ‘å€‘å‡è¨­é€™ä¸æ˜¯ Counter-poseï¼Œè€Œæ˜¯ MP çš„ Z è»¸åˆ¤æ–·éŒ¯èª¤ -> ç›¸ä¿¡é ­éƒ¨
        if is_conflict and abs_h_yaw > 40:
            final_yaw_direction_sign = 1 if h_yaw > 0 else -1
            
        # å‚™è¨»ï¼šå¦‚æœé ­è½‰ < 40 ä¸”è¡çªï¼Œæˆ‘å€‘ä¿ç•™ MP åŸåˆ¤ (è¦–ç‚º Counter-pose)ï¼Œé¿å…èª¤æ®º
            
        suffix = "Right" if final_yaw_direction_sign > 0 else "Left"
        
        # é–‹å§‹åˆ†é¡
        is_head_side = abs_h_yaw > THRES_HEAD_FRONT_LIMIT
        
        if not is_head_side:
            # èº«é«”å´ï¼Œé ­æ­£
            return f"Body_Turn_{suffix}_Face_Front"
        else:
            # èº«é«”å´ï¼Œé ­ä¹Ÿå´
            corrected_b_yaw = abs_b_yaw * final_yaw_direction_sign
            
            if (corrected_b_yaw * h_yaw) > 0: 
                # åŒå‘
                diff = abs_h_yaw - abs_b_yaw
                
                # [æ ¸å¿ƒä¿®æ­£] éå°ç¨±ä¸»å°æ¬Š
                # å·¦å´ gap é™è‡³ 6 (åŸ10)ï¼Œå³å´ç¶­æŒ 20
                dominance_gap = 20 if h_yaw > 0 else 6
                
                if diff > dominance_gap:
                    return f"Head_Turn_{suffix}"
                else:
                    return f"Side_View_{suffix}"
            else: 
                # åå‘
                return f"Head_Turn_{suffix}"

    # --- Priority 5: ç´”é ­è½‰ ---
    if abs_h_yaw > THRES_HEAD_PURE_TURN:
        return "Head_Turn_Right" if h_yaw > 0 else "Head_Turn_Left"

    # --- Priority 6: æ®˜é¤˜æ­ªé ­é¡ (Head Tilt) ---
    if h_roll > THRES_TILT: return "Head_Tilt_Left"
    if h_roll < -THRES_TILT: return "Head_Tilt_Right"

    # --- Priority 7: æ®˜é¤˜å‚¾æ–œé¡ (Body Lean) ---
    if b_roll > THRES_LEAN: return "Body_Lean_Right"
    if b_roll < -THRES_LEAN: return "Body_Lean_Left"

    # --- Priority 8: æ­£é¢é¡ ---
    if h_yaw > 15: return "Head_Slight_Right"
    if h_yaw < -15: return "Head_Slight_Left"
    
    return "Frontal"

# --- æ¨è«–èˆ‡è³‡æ–™è™•ç† ---
def run_inference(img_path, head_model, pose_estimator):
    """å›å‚³: é æ¸¬é¡åˆ¥, ä¹¾æ·¨çš„åœ–, è©³ç´°æ•¸æ“šå­—ä¸²"""
    if not os.path.exists(img_path):
        return "File Not Found", None, "ç„¡æ•¸æ“š"

    pil_img = Image.open(img_path).convert("RGB")
    w, h = pil_img.size
    img_arr = np.array(pil_img)
    draw = ImageDraw.Draw(pil_img)

    # 1. MediaPipe
    results = pose_estimator.process(img_arr)
    b_yaw, b_roll = None, 0.0
    bbox = None
    
    if results.pose_landmarks:
        lm = results.pose_landmarks.landmark
        b_yaw, b_roll = calc_angles(lm, w, h)
        bbox = get_face_box(lm, w, h)
        
        # ç•«éª¨æ¶
        l = lm[IDX_L_SHOULDER]
        r = lm[IDX_R_SHOULDER]
        if l.visibility > 0.5 and r.visibility > 0.5:
            lx, ly = int(l.x*w), int(l.y*h)
            rx, ry = int(r.x*w), int(r.y*h)
            draw.line([(lx, ly), (rx, ry)], fill="yellow", width=3)
            draw.ellipse((lx-5, ly-5, lx+5, ly+5), fill="red")
            draw.ellipse((rx-5, ry-5, rx+5, ry+5), fill="blue")

    # 2. SemiUHPE
    h_yaw, h_pitch, h_roll = 0.0, 0.0, 0.0
    if bbox and head_model:
        x1, y1, x2, y2 = bbox
        x1, y1 = max(0, x1), max(0, y1)
        x2, y2 = min(w, x2), min(h, y2)
        crop = pil_img.crop((x1, y1, x2, y2))
        
        if crop.size[0] > 10 and crop.size[1] > 10:
            tf = transforms.Compose([
                transforms.Resize((224, 224)),
                transforms.ToTensor(),
                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
            ])
            with torch.no_grad():
                input_t = tf(crop).unsqueeze(0).to(DEVICE)
                out = head_model(input_t)
                rot_mat = batch_torch_A_to_R(out).cpu().numpy()[0]
                r = Rotation.from_matrix(np.transpose(rot_mat))
                angles = r.as_euler("xyz", degrees=True)
                h_pitch = normalize_angle_180(angles[0] - 180)
                h_yaw = normalize_angle_180(angles[1])
                h_roll = normalize_angle_180(angles[2])
            
            draw.rectangle(bbox, outline="#00FF00", width=2)

    # 3. Classify
    delta = 0
    if b_yaw is not None:
        delta = abs(h_yaw - b_yaw)
        if delta > 180: delta = 360 - delta

    pred_class = classify_custom_priority(b_yaw, b_roll, h_yaw, h_pitch, h_roll, delta)

    data_text = (
        f"é æ¸¬é¡åˆ¥: {pred_class}\n"
        f"---------------------------\n"
        f"Body Yaw  : {b_yaw:.1f}" if b_yaw is not None else "Body Yaw  : N/A"
    )
    data_text += f"\nBody Roll : {b_roll:.1f}"
    data_text += f"\n---------------------------"
    data_text += f"\nHead Yaw  : {h_yaw:.1f}"
    data_text += f"\nHead Pitch: {h_pitch:.1f}"
    data_text += f"\nHead Roll : {h_roll:.1f}"
    data_text += f"\n---------------------------"
    data_text += f"\nDelta     : {delta:.1f}" if b_yaw is not None else "\nDelta     : N/A"
    
    return pred_class, pil_img, data_text

# --- è©•ä¼°é‚è¼¯ (å«èª¤åˆ¤åˆ†æ) ---
def evaluate_dataset(csv_path, checkpoint_path, progress=gr.Progress()):
    if not os.path.exists(csv_path):
        return None, "æ‰¾ä¸åˆ° CSV æª”æ¡ˆ", None, None

    head_model, pose_estimator = load_models(checkpoint_path)
    if not head_model:
        return None, "æ¨¡å‹è¼‰å…¥å¤±æ•—", None, None

    df = pd.read_csv(csv_path)
    mismatches = []
    
    total_count = 0
    correct_count = 0
    class_stats = {} 
    
    # è¨˜éŒ„èª¤åˆ¤æµå‘
    error_stats = {} 

    for idx, row in progress.tqdm(df.iterrows(), total=len(df), desc="Analyzing..."):
        img_path = row['Path']
        gt_pose = row['Pose']
        gt_sight = row.get('Sight', 'N/A')

        pred_pose, _, _ = run_inference(img_path, head_model, pose_estimator)

        if gt_pose not in class_stats:
            class_stats[gt_pose] = [0, 0] # [total, correct]
        class_stats[gt_pose][0] += 1
        total_count += 1

        if pred_pose == gt_pose:
            correct_count += 1
            class_stats[gt_pose][1] += 1
        else:
            mismatches.append({
                "idx": idx,
                "path": img_path,
                "gt_pose": gt_pose,
                "gt_sight": gt_sight,
                "pred_pose": pred_pose
            })
            
            if gt_pose not in error_stats:
                error_stats[gt_pose] = {}
            if pred_pose not in error_stats[gt_pose]:
                error_stats[gt_pose][pred_pose] = 0
            error_stats[gt_pose][pred_pose] += 1

    # --- 1. ç”¢ç”Ÿä¸»è¦çµ±è¨ˆè¡¨ ---
    stat_data = []
    for cls, vals in class_stats.items():
        acc = (vals[1] / vals[0]) * 100 if vals[0] > 0 else 0
        stat_data.append([cls, vals[0], vals[1], f"{acc:.1f}%"])
    
    total_acc = (correct_count / total_count) * 100 if total_count > 0 else 0
    stat_df = pd.DataFrame(stat_data, columns=["é¡åˆ¥ (GT)", "æ¨£æœ¬æ•¸", "æ­£ç¢ºæ•¸", "æ­£ç¢ºç‡"])
    stat_df = stat_df.sort_values(by="æ¨£æœ¬æ•¸", ascending=False)

    # --- 2. ç”¢ç”Ÿèª¤åˆ¤è©³ç´°åˆ†æè¡¨ (å„ªåŒ–ç‰ˆ) ---
    error_data = []
    for gt, preds in error_stats.items():
        gt_total = class_stats[gt][0]
        # è¨ˆç®—è©²é¡åˆ¥çš„ç¸½èª¤åˆ¤æ•¸ï¼Œæ–¹ä¾¿æ’åº
        total_errors_for_class = sum(preds.values())
        
        for pred, count in preds.items():
            rate = (count / gt_total) * 100
            # æˆ‘å€‘åŠ å…¥ä¸€å€‹éš±è—æ¬Šé‡(total_errors_for_class)ä¾†æ’åºï¼Œè®“éŒ¯èª¤æœ€å¤šçš„é¡åˆ¥æ’å‰é¢
            error_data.append({
                "GT": gt,
                "Pred": pred,
                "Count": count,
                "Rate": f"{rate:.1f}%",
                "_sort_key": total_errors_for_class # è¼”åŠ©æ’åºç”¨
            })
    
    # è½‰æˆ DataFrame ä¸¦æ’åºï¼šå…ˆçœ‹å“ªå€‹é¡åˆ¥éŒ¯èª¤ç¸½æ•¸æœ€å¤šï¼Œå†çœ‹è©²é¡åˆ¥å…§å“ªå€‹èª¤åˆ¤æœ€å¤š
    error_df_raw = pd.DataFrame(error_data)
    if not error_df_raw.empty:
        error_df_raw = error_df_raw.sort_values(by=["_sort_key", "Count"], ascending=[False, False])
        # ç§»é™¤è¼”åŠ©æ’åºçš„ key
        error_df = error_df_raw[["GT", "Pred", "Count", "Rate"]]
        error_df.columns = ["çœŸå¯¦é¡åˆ¥ (GT)", "è¢«èª¤åˆ¤ç‚º (Predicted)", "èª¤åˆ¤æ•¸é‡", "èª¤åˆ¤ä½”æ¯” (ä½”è©²GTç¸½æ•¸)"]
    else:
        error_df = pd.DataFrame(columns=["çœŸå¯¦é¡åˆ¥ (GT)", "è¢«èª¤åˆ¤ç‚º (Predicted)", "èª¤åˆ¤æ•¸é‡", "èª¤åˆ¤ä½”æ¯” (ä½”è©²GTç¸½æ•¸)"])

    summary_text = f"ç¸½æ¨£æœ¬æ•¸: {total_count} | ç¸½æ­£ç¢ºç‡: {total_acc:.2f}% | éŒ¯èª¤æ¡ˆä¾‹: {len(mismatches)} å¼µ"
    
    return mismatches, summary_text, stat_df, error_df

# --- Gradio UI ---
def on_load_click(csv_file, ckpt_file):
    # æ›´æ–°å›å‚³å€¼ï¼Œå¤šæ¥æ”¶ä¸€å€‹ error_df
    mismatches, summary, stat_df, error_df = evaluate_dataset(csv_file, ckpt_file)
    
    if not mismatches:
        return gr.update(visible=True), summary, stat_df, error_df, None, None, None, None, [], 0
    
    first_case = mismatches[0]
    _, img_l, info_l, img_r, info_r = load_mismatch_case(first_case, ckpt_file)
    return gr.update(visible=True), summary, stat_df, error_df, img_l, info_l, img_r, info_r, mismatches, 0

def load_mismatch_case(case_data, ckpt_path):
    global Global_Head_Model, Global_Pose_Estimator
    if 'Global_Head_Model' not in globals() or Global_Head_Model is None:
         Global_Head_Model, Global_Pose_Estimator = load_models(ckpt_path)
    
    path = case_data['path']
    gt_pose = case_data['gt_pose']
    gt_sight = case_data['gt_sight']
    
    img_left = Image.open(path).convert("RGB") if os.path.exists(path) else None
    info_left = f"File: {os.path.basename(path)}\n\n[GT]\nPose: {gt_pose}\nSight: {gt_sight}"
    pred_pose, img_right, debug_text = run_inference(path, Global_Head_Model, Global_Pose_Estimator)
    
    return 0, img_left, info_left, img_right, debug_text

def nav_click(direction, mismatch_list, current_idx, ckpt_path):
    if not mismatch_list: return None, None, None, None, current_idx
    new_idx = max(0, min(current_idx + direction, len(mismatch_list) - 1))
    case = mismatch_list[new_idx]
    _, img_l, info_l, img_r, info_r = load_mismatch_case(case, ckpt_path)
    return img_l, info_l, img_r, info_r, new_idx

with gr.Blocks(title="æ¨™è¨» vs å·¥å…· å·®ç•°åˆ†æå™¨ (V-Final-Optimized)") as demo:
    state_mismatches = gr.State([])
    state_idx = gr.State(0)

    gr.Markdown("## ğŸ” GT vs Tool Analysis (V-Final-Optimized Logic)")
    with gr.Row():
        inp_csv = gr.Textbox(label="CSV è·¯å¾‘", value="/media/will/æ–°å¢ç£ç¢Ÿå€/dataset/DeepFashion-MultiModal/1_1_final/labels.csv")
        inp_ckpt = gr.Textbox(label="æ¬Šé‡è·¯å¾‘", value=CHECKPOINT_PATH)
        btn_start = gr.Button("ğŸš€ é–‹å§‹", variant="primary")

    lbl_summary = gr.Textbox(label="ç¸½çµ", interactive=False)
    
    with gr.Column(visible=False) as result_area:
        
        # --- ä¸ŠåŠéƒ¨ï¼šå·¦å³åˆ†æ¬„ (çµ±è¨ˆ vs äº’å‹•çœ‹åœ–) ---
        with gr.Row():
            # å·¦å´ï¼šä¸»è¦çµ±è¨ˆ (åªæ”¾ç¸½è¡¨ï¼Œæ¯”è¼ƒçŸ­)
            with gr.Column(scale=1):
                gr.Markdown("### ğŸ“Š æ•´é«”çµ±è¨ˆ")
                df_stats = gr.Dataframe(label="æ­£ç¢ºç‡çµ±è¨ˆ", interactive=False)
            
            # å³å´ï¼šåœ–ç‰‡äº’å‹•å€
            with gr.Column(scale=2):
                with gr.Row():
                    btn_prev = gr.Button("â¬…ï¸ Prev")
                    lbl_idx = gr.Label(value="0", show_label=False)
                    btn_next = gr.Button("Next â¡ï¸")
                with gr.Row():
                    with gr.Column():
                        img_gt = gr.Image(label="GT Image", type="pil", height=500)
                        txt_gt = gr.Textbox(label="GT Info", lines=6)
                    with gr.Column():
                        img_tool = gr.Image(label="Tool Debug (Clean)", type="pil", height=500)
                        txt_tool = gr.Textbox(label="Tool Data", lines=10)

        # --- ä¸‹åŠéƒ¨ï¼šå…¨å¯¬èª¤åˆ¤åˆ†æ (ç§»åˆ°é€™è£¡ï¼) ---
        gr.Markdown("### ğŸ“‰ èª¤åˆ¤è©³ç´°åˆ†æ")
        with gr.Accordion("é»æ“Šå±•é–‹è©³ç´°èª¤åˆ¤åˆ—è¡¨", open=True): # é è¨­å±•é–‹æˆ–æ”¶èµ·çš†å¯
            # é€™è£¡ä½¿ç”¨å…¨å¯¬ï¼Œæ¬„ä½ä¸æœƒå†è¢«å£“ç¸®
            df_errors = gr.Dataframe(
                label="èª¤åˆ¤æµå‘çŸ©é™£ (GT -> Pred)", 
                headers=["çœŸå¯¦é¡åˆ¥ (GT)", "è¢«èª¤åˆ¤ç‚º (Predicted)", "èª¤åˆ¤æ•¸é‡", "èª¤åˆ¤ä½”æ¯” (ä½”è©²GTç¸½æ•¸)"],
                interactive=False,
                wrap=True # è®“æ–‡å­—è‡ªå‹•æ›è¡Œï¼Œé˜²æ­¢éé•·
            )

    # äº‹ä»¶ç¶å®š (ç¶­æŒä¸è®Š)
    btn_start.click(on_load_click, [inp_csv, inp_ckpt], [result_area, lbl_summary, df_stats, df_errors, img_gt, txt_gt, img_tool, txt_tool, state_mismatches, state_idx])
    btn_prev.click(lambda m, i, c: nav_click(-1, m, i, c), [state_mismatches, state_idx, inp_ckpt], [img_gt, txt_gt, img_tool, txt_tool, state_idx])
    btn_next.click(lambda m, i, c: nav_click(1, m, i, c), [state_mismatches, state_idx, inp_ckpt], [img_gt, txt_gt, img_tool, txt_tool, state_idx])
    state_idx.change(lambda i, m: f"Case {i+1} / {len(m)}", [state_idx, state_mismatches], lbl_idx)

if __name__ == "__main__":
    demo.queue().launch(server_name="0.0.0.0", server_port=7861)